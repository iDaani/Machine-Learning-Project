---
title: "Assignment 3: Machine Learning"
author: "Pranay Meshram (40280938)"
output:
  pdf_document:
     includes: 
         in_header: "preamble.tex"
  html_notebook: default
  html_document:
    df_print: paged
---



## Introduction

This assignment will focus on the machine learning models to classify between different types of images such as letters, non-letters, finding out the accuracy of models. This assignment will also put focus on cross-validating the data and recording the accuracy.

# Section 1

```{r echo=TRUE, fig.show='hide', message=FALSE, warning=FALSE}
setwd("C:/Users/Asus/Desktop/AI Assignment")

source("./section1_code.r")
```

Now we will first initially install different libraries as per needed. 

We install caTools library for use of splitting samples.

```{r}
library(caTools)
```

Then we install InformationValue library for use in ConfusionMatrix

```{r}
library(InformationValue)
```

We install modelr and MLeval library for running our machine learning model.

```{r}
library(modelr)

library(MLeval)
```

And finally, the ROCR library to plot the ROC curve

```{r}
library(ROCR)
```


Once we're set installing the library, we set the seed level to 42 and read all the features from the '40280938_features.csv' file.

```{r}
features <- read.csv(file = './40280938_features.csv', header = TRUE)

nrow(features)
```

As we can see, we have 140 rows of data in our features files.


## Section 1.1

Now, we have our data of letters from 'a' to 'j' and then we have the other non-letters. We can see that from rows 81 till 140, all the images are non-letters.

```{r}
features[80,]

features[81,]
```


We will use this data to classify as letters and non-letters. Rows from 1 to 80 are letters while rows from 80 to 140 are non-leters.

We will create a separate column **isLetter** with two values, 1 and 0. 1 means the image is a letter while 0 means the image is a non-letter. 

```{r}
features$isLetter <- sample(100, size = nrow(features), replace = TRUE)

features$isLetter[1:80] = 1
features$isLetter[81:140] = 0
```


We then split the data into a training set and a test set. We put 80% of the data (0.8) in training set and the rest in test set.

```{r}
i_split<- sample.split(row.names(features), 0.8)
train<-features[i_split, ]
test<-features[!i_split, ]

nrow(train)
nrow(test)
```
Once we're done splitting the data, we fit a logistic regression model using the training data. We try to predict if an image is a letter or not based on **nr_pix** and **aspect_ratio** features of the data.

```{r}
logistic <- glm(isLetter ~ nr_pix + aspect_ratio, data = train, family="binomial")

logistic
```

Here we an see that there is a very modest change in deviance.

We now predict the probability:

```{r}
probability = predict(logistic, test, type="response")
probability
```

As we can see, most of the probabilities are very close to 50%. Some are as high as 70% which means our model is pretty good at predicting the possibility if an image belongs to the letter category.

We now report the confusion matrix for threshold of 0.5:

```{r}
confusion_matrix <- confusionMatrix(test$isLetter, probability, threshold = 0.5)
confusion_matrix
```

Based on this Confusion Matrix, we can make certain notes:

Here, 

True Negatives (TN) : 3
False Positives (FP) : 5
True Positives (TP) : 10
False Negatives (FN) : 10
Actual No: 8
Actual Yes: 20
Predicted No: 13
Predicted Yes: 15
Total number of items: 28


**Accuracy** - We know that accuracy is (TP + TN) / Actual No

```{r}
accuracy <- (10+3)/28
accuracy
```
The accuracy of this model isn't good as it's below 50% but closer to it.

**True Positive Rate** - We can calculate the true positive rate by using 'sensitivity' function

```{r}
true_positive_rate <- sensitivity(test$isLetter, probability)
true_positive_rate
```
Our model is good predicting if a letter is actually a letter when it's a letter with positive rate above 50.

**False Positive Rate** - False positive rate is FP / Actual No

```{r}
false_positive_rate <- (5/8)
false_positive_rate
```
Our model is good predicting if a non-letter is actually a non-letter when it's a non-letter with positive rate above 50.


**Precision** - Precision is TP/Predicted Yes

```{r}
precision <- (10/15)
precision
```
Our model is pretty good at being correct in predicting with a decent precision rate of 66%.


**Recall** - Recall is TP/ (TP + FN)

```{r}
recall <- 10/ (10+10)
recall
```

Our model has average recall of 50%.


**F1-Score** - F1 score is (2 * Precision * Recall) / (Precision + Recall)

```{r}
f1_score <- (2*precision*recall)/(precision+recall)
f1_score
```

A poor F1 score is of 0.0 and our F1 score is 0.5 which is on average, providing good accuracy of the our model's performance.


## Section 1.2

Here we load the caret library first for 'trainControl' method to work:

```{r warning=FALSE, include=FALSE, results='hide'}
library(caret)
```

```{r eval=FALSE, warning=FALSE, include=TRUE, results='hide'}
library(caret)
```


Now we are going to perform k-fold cross validation on the previous model. We divide dataset into k groups and then repeat the process k times with each set.

We first perform the cross validation here:

```{r}
ctrl <- trainControl(method = "cv", number = 5, savePredictions = T)

```

Then we train the model same as how we did in 1.1. However, this time we train all the 140 items instead of just the training data. There's no seperate test data.

```{r echo=TRUE, message=TRUE, warning=FALSE}
model <- train(isLetter ~ nr_pix + aspect_ratio, data = features, method = "lm", trControl = ctrl)

model
```

Here we can make a few note from the observations.

RMSE is pretty low, which is good for the model as lower the RMSE, the more closely can our model predict actual observations.

RSquared is pretty low, which leads to inaccurate predictions as higher the R-Squared, more closely a model predicts actual observations.

MAE is pretty low which is good for prediction, as lower the MAE, more closely the model predicts actual observations.

Now we predict the probabilities:

```{r}
probability_cross_validated_data <- predict(model, features)
```

We then make a confusion matrix. Here, we'll have to unload the caret library first for 'confusionMatrix' function to work as some methods of both the libraries are same in name and causees errors.

```{r}
detach("package:caret", unload = TRUE)

confusionMatrix(features$isLetter, probability_cross_validated_data, threshold = 0.5)


```

True Negatives (TN) : 20
False Positives (FP) : 18
True Positives (TP) : 62
False Negatives (FN) : 40
Actual No: 38
Actual Yes: 102
Predicted No: 60
Predicted Yes: 80
Total number of items: 140

**Accuracy**

```{r}
accuracy_cross_validated_data <- (62+20)/140
accuracy_cross_validated_data
```
Our data is pretty accurate in this cross-validated model, as expected because of cross validation.

**True Positive Rate**

```{r}
true_positive_rate_cross_validated_data <- sensitivity(features$isLetter, probability_cross_validated_data)

true_positive_rate_cross_validated_data
```
**False Positive Rate**

```{r}
false_positive_rate_cross_validated_data <- (18/38)
false_positive_rate_cross_validated_data
```

**Precision**

```{r}
precision_cross_validated_data <- (62/80)
precision_cross_validated_data
```

**Recall**

```{r}
recall_cross_validated_data <- 62/ (62+40)
recall_cross_validated_data
```
**F1-Score**

```{r}
f1_score_cross_validated_data <- (2*precision_cross_validated_data*recall_cross_validated_data)/(precision_cross_validated_data+recall_cross_validated_data)

f1_score_cross_validated_data
```


Our model provides good accuracy, more than the model without cross-validated accuracy. It leads to increase in every rate that we previously calculated in the previous model, which makes sense as k-fold cross validation would increase the accuracy of the model.

## Section 1.3

Now we plot the ROC curve for Section 1.2.

```{r}
rocr_pred = prediction(probability_cross_validated_data, features$isLetter)

rocr_performance <- performance(rocr_pred, "tpr", "fpr")

plot(rocr_performance)

plot(rocr_performance, colorize = TRUE)


```
This is a decent classifier, not perfect.  It has chance level specificty and sensitivity but it's not completely straight and with high false positive rate, the true positive rate increases as well.

At threshold, we will not collect any poor case however we ill correctly label good care cases. At the end, we will catch all poor care cases and incorrectly label all good care cases as poor cases.

However since the line is not straight, we can say that our model is not a random classifier. It is a decent classifier but not perfect


# Section 2

We first set the working directory and the source code.

```{r eval=FALSE, include=TRUE}
setwd("C:/Users/Asus/Desktop/AI Assignment")

source("./section2_code.r")
```



```{r fig.show='hide', message=FALSE, warning=FALSE, include=FALSE}
setwd("C:/Users/Asus/Desktop/AI Assignment")

source("./section2_code.r")
```

We first install the library to use knn model and then we import the data as features

```{r}
library(class)

features <-  read.csv(file = './40280938_features.csv', header = TRUE)

```

Now, we only choose a, j and no_letters as per the assignment's specification. We know that a letters are till 1st to 8th row, j from 73rd to 80th and non-letters from 81st to 140th row.

```{r}
a_letters <- features[1:8,]
j_letters <- features[73:80,]
all_non_letters <- features[81:140,]
```

Once done above, we add all the above data in a separate new dataframe. This will be used throughout the second section.

```{r}
new_features <- rbind(a_letters, j_letters, all_non_letters)

```

Since we'll be using 'label' for KNN classification, we make 'label' column as a categorical factor. a' and 'j' are classified as 'letter' while 'sad', 'smiley' and 'xclaim' are classified separetly for four-way classification.

```{r}
levels = c("a","j", "sad", "smiley", "xclaim")

labels = c("letter","letter","sad", "smiley", "xclaim" )


new_features$label <- factor(new_features$label,
                        levels = levels, labels = labels)


str(new_features)


```

We then set the seed to 42

```{r}
set.seed(42)
```


We print out the summary and sample head first:

```{r}
summary(new_features)
head(new_features, 10)
```

Afterwards we shuffle the data.



```{r}
new_features <- new_features[sample(nrow(new_features)),]

new_features

head(new_features, 10)
```

We then have to splice the data into training set and test set.

However here we use the entire data as the training data and there's no separate test set. So we use '1.0' value for size. We randomly select 100% of data.

```{r}
new_features_fold <- sample(1:nrow(new_features),size=nrow(new_features)*1,replace = FALSE) #random selection of 100% data.

```


We then select the features for classification:

```{r}
fs = c('nr_pix', 'aspect_ratio', 'rows_with_1', 'cols_with_1')

```


We used the above four features as in the previous assignment, those features were very helpful is discriminating between different types of images. They had the most varied values and were properly calculated amongst other features. We need features with higher variability and precision to determine accurate classification.


## Section 2.1

We create a vector to store all accuracies in:

```{r}
accs_2_1=c()
```


Then we put all the separate odd values of k from 1 to 13 to go through:

```{r}
knn_values <- c(1, 3, 5, 7, 9, 11, 13)
```


We perform our knn classification (We won't print individual results as they're too many and big to report in the report):

```{r results='hide'}
for (i in knn_values) {
  
  train_items_this_fold  = new_features[new_features_fold,] 
  validation_items_this_fold = new_features[new_features_fold,]
  
  # fit knn model on this fold
  
  predictions = knn(train_items_this_fold[,fs], 
                    validation_items_this_fold[,fs], 
                    train_items_this_fold$label, k=i)
  predictions
  
  correct_list = predictions == validation_items_this_fold$label
  nr_correct = nrow(validation_items_this_fold[correct_list,])
  
  acc_rate = nr_correct/nrow(validation_items_this_fold)
  accs_2_1[i] = acc_rate
  print(acc_rate)
}


```
```{r}
accs_2_1 <- accs_2_1[!is.na(accs_2_1)]

accs_2_1
```
We report the accuracy over the full set of 76 items ran by our knn_classification model by the odd values of k.

We report high accuracy in our model as all the accuracies have been above 60%, thus noting that our model is good in performing four way classification.


## Section 2.2


In this section, we run the same model as in Section 2.1 However, we cross-validate it with 5-fold cross validation first.

As usual, we put k values to go through in a variable along with a new vector to store all accuracies of this model in:

```{r}
knn_values <- c(1, 3, 5, 7, 9, 11, 13)

accs_2_2=c()

```


We then perform the model:

```{r results='hide'}
library(caret)

for (i in knn_values) {
  
  # define training control
  train_control <- trainControl(method="cv", number=5)
  
  tune_grid <- expand.grid(k = i)
  
  # train the model
  model <- train(label~nr_pix+aspect_ratio+cols_with_1+rows_with_1, data=new_features, 
                 trControl=train_control, tuneGrid=tune_grid, method="knn")
  # summarize results
  print(model)
  
  # Store accuracy rate
  
  accs_2_2[i] = model$results$Accuracy
  
}
```

We have to hide the results as the amount of times this model will run is too many.

We print out the accuracy rate after removing NA values that come with adding it to a vector:

```{r}
accs_2_2 <- accs_2_2[!is.na(accs_2_2)]

accs_2_2
```
Oddly, we can see that accuracy has decreased for each value of k in comparison to classification without cross-validation. The model is not as accurate with cross-validation.


## Section 2.3

We can see that the value of k = 1 has the highest accuracy with 66%. So we will use that for this section.

We run the model again but this time, not with different values. With just k = 3, in order to summarize it in the form of a confusion matrix.

```{r}
# define training control
train_control_3 <- trainControl(method="cv", number=5)

tune_grid_3 <- expand.grid(k = 1)

# train the model
model_3 <- train(label~nr_pix+aspect_ratio+cols_with_1+rows_with_1, data=new_features, 
               trControl=train_control_3, tuneGrid=tune_grid_3, method="knn")
# summarize results
print(model_3)

```

```{r}
confusionMatrix(model_3)
```
Discriminating letter with sad images, sad with exclamatory images, exclamatory with sad and exclamatory with smiley are the most difficulty pairs to discriminate from.


## Section 2.4

For this we first install the ggplot library.

```{r}
library(ggplot2)
```


Then we make a separate dataframe which includes all the accuracies and the k-values:

```{r}
plotting_data_frame <- data.frame(accs_2_1,accs_2_2, knn_values)

plotting_data_frame
```

Then make a graph using ggplot function and plot it:

```{r}
plot_2 <- ggplot()+
  geom_line(data=plotting_data_frame,aes(y=accs_2_1,x= 1/knn_values,colour="Training Accuracy Rate"),size=1, linetype="dashed" )+
  geom_line(data=plotting_data_frame,aes(y=accs_2_2,x= 1/knn_values,colour="Cross Validated Accuracy Rate"),size=1, linetype="dashed") +
  scale_color_manual(name = "Legend", values = c("Training Accuracy Rate" = "darkorange", "Cross Validated Accuracy Rate"= "darkgreen")) + labs(y= "Accuracy Rate", x = "1/K values")

plot_2
```

Here, we have plotted accuracy rate of training data and cross validated data against 1/K. We can see that as 1/K increases, the accuracy rate increases. There was a steady rise before K value (K value = 4 at maximum - in our data, we used odd values of K from 1 to 13) and then decline till just before halfway of maximum k value = 2. Then K value increases, with increase accuracy and declining error rate.



# Section 3


```{r eval=FALSE, include=TRUE}
setwd("C:/Users/Asus/Desktop/AI Assignment")

source("./section3_code.r")
```



```{r fig.show='hide', message=FALSE, warning=FALSE, include=FALSE}
setwd("C:/Users/Asus/Desktop/AI Assignment")

source("./section3_code.r")
```


We first set the directory listing for CSV files:

```{r}
imagesDir<-"./Images"


imagesDir
```
Then we put all the features into a separate variable:

```{r}
allData <- read.table("./Images/all_features.csv")
allData
```

We then set the column names of the data:

```{r}
colnames(allData) <- c("label", "index", "nr_pix", "rows_with_1", "cols_with_1",
                       "rows_with_3p", "cols_with_3p", "aspect_ratio", "neigh_1",
                       "no_neigh_above", "no_neigh_below", "no_neigh_left", "no_neigh_right",
                       "no_neigh_horiz", "no_neigh_vert", "connected_areas", "eyes",
                       "custom")
```


We make 'label' as categorical factor as we'll be classifying our data based on it. All the letters from 'a' to 'j' classified as letters, and others based on their emoticons.

```{r}
 levels = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "sad", "smiley", "xclaim")

 labels = c("letter","letter","letter","letter","letter","letter",
          "letter","letter","letter","letter", "sad", "smiley", "xclaim" )


allData$label <- factor(allData$label,
                        levels = levels, labels = labels)

str(allData)

```

here, we choose 80 values of each of the 13 image types as training data and the rest as test data. Also, we have removed the custom feature as it was stated in the assignment specification.

```{r}
training_data <- allData[allData$index<80, 1:17]

test_data <- allData[allData$index >= 80, 1:17]

```


## Section 3.1


Here we will perform classification with random forests using 5-fold cross validation.

We first install the libraries needed and set seed:

```{r}
library(caret)
library(randomForest)
set.seed(42)

```


We first do the cross-validation. We perform a grid search of two hyper-parameters for our cross validation.

```{r}
train_control = trainControl(method="cv", number = 5, search = "grid", savePredictions = T)

set.seed(42)

tuningGrid <- expand.grid(mtry=c(2,4,6,8))
```

Now we will use random-forests with number of trees between 25 and 375 at increment of 50, and number of parameters individually at 2, 4, 6 and 8.



```{r}
set.seed(42)

# For number of trees = 25

model_25 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 25)

model_25

model_25$finalModel$ntree


# For number of trees = 75

model_75 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 75)

model_75

model_75$finalModel$ntree


# For number of trees = 125

model_125 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 125)

model_125

model_125$finalModel$ntree


# For number of trees = 175

model_175 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 175)

model_175

model_175$finalModel$ntree

# For number of trees = 225

model_225 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 225)

model_225

model_225$finalModel$ntree

# For number of trees = 275

model_275 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 275)

model_275

model_275$finalModel$ntree


# For number of trees = 325

model_325 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree =325)

model_325

model_325$finalModel$ntree


# For number of trees = 375

model_375 = train(label~., data=training_data, method="rf", trControl = train_control,
              tuneGrid = tuningGrid, ntree = 375)

model_375
model_375$finalModel$ntree
```
Now the model with the highest accuracy is the second last model with number of trees = 325 and predictor number as 2. Hence it's the best model.

```{r}
model_highest_accuracy = 0.8942308
```

We plot this model!

```{r}
plot.train(model_325)
```

From the graph, it's visible that the random selected features we chose using cross-validation are 2, 4, 6 and 8.

The best predictor number is 2 for this model and apparently from the graph, the more number of predictor numbers we have, there's a chance of more redundancy predictor numbers selected as split nodes. Hence that's the reason for decrease of predictions.

## Section 3.2

In this section, we'll use the best model from the previous section. The model with Nt = 325. We first set the cross-validation to 5-fold.

```{r}
train_control_refitted = trainControl(method="cv", number = 5, search = "grid", savePredictions = T)



tuningGrid_refitted <- expand.grid(mtry=c(2,4,6,8))
```

We then create a vector to store all the accuracies of the model in:

```{r}
all_accuracies <- c()

all_accuracies

```

Now, we refit the model 15 times. That means we run the same model 15 times and note the highest accuracy to the 'all_accuracies' vector.

```{r}
for (i in seq(1,15,1)) {

model_325 = train(label~., data=training_data, method="rf", trControl = train_control,
                  tuneGrid = tuningGrid, ntree = 325)

all_accuracies <- append(all_accuracies, max(model_325$results$Accuracy))


}
```

Then we print all the accuracies:

```{r}
print(all_accuracies)
```
We get the mean of all accuracies:

```{r}
mean_of_accuracies <- mean(all_accuracies)
mean_of_accuracies
```

We get the standard deviation of all accuracies:

```{r}
standard_deviation_of_accuracies <- sd(all_accuracies)
standard_deviation_of_accuracies
```
Our standard deviation is significantly low. This means that we're confident of our estimators of the model.

Now we check if our model performs better than chance.

We perform a one-sample t-test.

```{r}
t.test(all_accuracies)
```
Since the value of p is very low, we know that our model performed didn't have any chance in it and performed significantly better than chance.


